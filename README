NLP_Assignment6
===============

Problem 1:
----------

a)

i. The new probability that day 1 is hot, or p(->H) is 0.491
ii. If day 1 is 2 ice creams, p(->H) for day 2 is 0.977. With 1 ice cream, p(->) for day 2 is 0.918. So the modification only changes the probability of day 2 being hot by -0.059.
iii. If we eat 2 ice creams on day 1, the final graph gives days 1-5 a probability of almost 1 of being hot. However, if we change it to 1 ice cream, the final graph gives day 1 a 0.0 probability of being hot, and days 2 and 3 have significantly reduced probabilities as well at 0.557 and 0.813 respectively.

b)

i. Before the change, days 11-13 in the initial graph have fairly high probabilites of being hot at 0.752, 0.856, and 0.779 respectively. After the change, they get reduced to 0.0, 0.441, and 0.441. Since we have changed our model to say that there is 0 change of a day being hot if there was 1 ice cream eaten, the probability of day 11 is brought all the way down, which in turn plays a large influence on days 12 and 13 especially since 14, 15, and 16 also have 0 probability.
ii. The final graph is actually almost identical in both cases. This is because even without the bias of p(1|H) = 0, our model will eventually learn p(1|H) = 1.6E-4 which for all practical purposes has the same effect on the graph. Therefore by starting with p(1|H) = 0, we're actually not doing anything that the model would not do by itself after 10 iterations.
iii. After 10 iterations with the new probabilities, p(1|H) is still 0. This is because at each step, p(1|H) is set equal to p(->H, 1) / p(->H) from the previous iteration. But since p(1|H) is 0, p(->H, 1) is also 0 at every iteration, so no matter what p(->H) is, p(1|H) will always be set to 0.

c)

i. The alpha value that represents the probability of all the ways to reach that sequence, the alpha value of the EOS character will be the probability of all possible parses of the sentence.
ii. An H constituent represents a hot day on the day that corresponds to the depth of the consituent. H -> 1 C has the probability of p(1 | C) * p(C | H) since it needs to capture the probability of a cold day and 1 ice cream following a hot day. The probability of H -> ep translates to p(STOP | H). The approach on the right is in Chomsky Normal Form. Instead of having to compute p(H -> 1 C) which is a bit awkward, we can compute p(H -> EC C) = p(C | H) and p(1 | EC) = p(1 | C) which can be read directly from our table.


Problem 2:
----------

See vtag


Problem 3:
----------

See vtag


Problem 4:
----------

After applying the one-count smoothing, I got the following results:

Tagging accuracy (Viterbi decoding): 94.09% (known: 96.82% novel: 65.80%)
Perplexity per Viterbi-tagged test word: 864.905

This represents an improvement of 1.61 percentage points over the baseline in overall accuracy, 0.83 points in known word accuracy, and 9.73 points in novel word accuracy.

We also improved by 712.594 in perplexity, which is very substantial.


Problem 5:
----------

After adding posterior decoding, I got the following results for entest:

Tagging accuracy (Viterbi decoding): 94.09% (known: 96.81% novel: 65.84%)
Perplexity per Viterbi-tagged test word: 864.609
Tagging accuracy (posterior decoding): 94.16% (known: 96.84% novel: 66.41%

As we can see, the posterior decoder does just marginally better overall, with hardly any difference for known words, and about a 0.57 percentage point improvement for novel words. The program as a whole runs a bit slower but not by enough to suggest that the posterior decoding is taking significantly longer than the Viterbi tagging.

On the ice-cream set, I noticed that my implementation gives exactly the same output as the Excel sheet on the first iteration. However, this output is actually not as good as the Viterbi tagger, oddly enough. It seems that this is an outlier possibly caused by a small data set, though, as forwards-backwards did outperform Viterbi on the entest data.


Problem 6:
----------

I get the following output after running on enraw with entrain25k:

Tagging accuracy (Viterbi decoding): 94.17% (known: 96.54% seen: 62.86% novel: 65.65%)
Perplexity per Viterbi-tagged test word: 960.147

Iteration 0: Perplexity per untagged raw word: 1007.43

Tagging accuracy (Viterbi decoding): 94.28% (known: 96.52% seen: 68.64% novel: 66.84%)
Perplexity per Viterbi-tagged test word: 941.445

Iteration 1: Perplexity per untagged raw word: 613.30

Tagging accuracy (Viterbi decoding): 94.19% (known: 96.53% seen: 68.74% novel: 65.59%)
Perplexity per Viterbi-tagged test word: 936.853

Iteration 2: Perplexity per untagged raw word: 609.13

Tagging accuracy (Viterbi decoding): 94.21% (known: 96.55% seen: 68.78% novel: 65.70%)
Perplexity per Viterbi-tagged test word: 935.864

Iteration 3: Perplexity per untagged raw word: 608.55

a)

Since we must start with a ###, we know that the total probability over all paths from 0 to 0 that end in state ### must be 1, since that's the only possible state to be in at word 0.

Similarly we must end with a ###, so the total probability over all paths from n to n that end in state ### must be 1 as well.

b)

The Viterbi perplexity is higher because it is computed directly from the probability of getting that sentence with that particular tagging. The posterior perplexity comes from the probability of generating that sentence without regard to tagging (there is none). Since the sentence with the given tagging is a special case of the sentence, it makes sense that it would have a lower probability of being generated, and thus the Viterbi perplexity would be higher as a result.

c)

This would be cheating. We are not allowed to know anything about the test data in a way that could influence our predictions. If we included test data in V, we wouldn't even encounter any OOV words, and the purpose of the test data would be lost.

d)

Not every iteration improved the overall accuracy. That being said, after 3 iterations, the overall accuracy made net improvement.

The same is true of the known, and novel counts.

However, the seen count improved after each iteration. This could be either a coincidence, or an actual improvement in the model, since at each iteration we get better at predicting the raw data, and the seen words are tagged based on what we computed in raw.

e)

EM helped by allowing us to extrapolate additional training data from an unlabeled data set. By getting initial probabilities from train, we could make predictions on raw, and then use those predictions to get better initial probabilities. By iterating multiple times, we improved our accuracy on the raw data, and made it even more useful.

f)

EM's ability to predict is based on the initial probabilities it is fed. If for whatever reason the probs from train and test did not closely match the probs from raw, we would end up hurting ourselves by trying to use raw as evidence. In other words, if raw is a less generalizable dataset than train, it won't help us and could hurt us.

The data are also not perfect. A little bit of natural variance could cause EM to get incorrect probabilities. If the smoothing is not strong enough, it could overfit the data and not generalize well to the test set.

g)

I don't believe that I have ever eaten more than 1 ice cream in a single day. It's possible that I may have eaten 2 on some forgotten occasion. I tend to have ice cream fairly rarely as I cannot eat eggs and about 50% of all ice creams are made with eggs. That being said, I do really like it.
