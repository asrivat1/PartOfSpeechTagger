NLP_Assignment6
===============

Problem 1:
----------

a)

i. The new probability that day 1 is hot, or p(->H) is 0.491
ii. If day 1 is 2 ice creams, p(->H) for day 2 is 0.977. With 1 ice cream, p(->) for day 2 is 0.918. So the modification only changes the probability of day 2 being hot by -0.059.
iii. If we eat 2 ice creams on day 1, the final graph gives days 1-5 a probability of almost 1 of being hot. However, if we change it to 1 ice cream, the final graph gives day 1 a 0.0 probability of being hot, and days 2 and 3 have significantly reduced probabilities as well at 0.557 and 0.813 respectively.

b)

i. Before the change, days 11-13 in the initial graph have fairly high probabilites of being hot at 0.752, 0.856, and 0.779 respectively. After the change, they get reduced to 0.0, 0.441, and 0.441. Since we have changed our model to say that there is 0 change of a day being hot if there was 1 ice cream eaten, the probability of day 11 is brought all the way down, which in turn plays a large influence on days 12 and 13 especially since 14, 15, and 16 also have 0 probability.
ii. The final graph is actually almost identical in both cases. This is because even without the bias of p(1|H) = 0, our model will eventually learn p(1|H) = 1.6E-4 which for all practical purposes has the same effect on the graph. Therefore by starting with p(1|H) = 0, we're actually not doing anything that the model would not do by itself after 10 iterations.
iii. After 10 iterations with the new probabilities, p(1|H) is still 0. This is because at each step, p(1|H) is set equal to p(->H, 1) / p(->H) from the previous iteration. But since p(1|H) is 0, p(->H, 1) is also 0 at every iteration, so no matter what p(->H) is, p(1|H) will always be set to 0.

c)

i. The alpha value that represents the probability of all the ways to reach that sequence, the alpha value of the EOS character will be the probability of all possible parses of the sentence.
ii. An H constituent represents a hot day on the day that corresponds to the depth of the consituent. H -> 1 C has the probability of p(1 | C) * p(C | H) since it needs to capture the probability of a cold day and 1 ice cream following a hot day. The probability of H -> ep translates to p(STOP | H). The approach on the right is in Chomsky Normal Form. Instead of having to compute p(H -> 1 C) which is a bit awkward, we can compute p(H -> EC C) = p(C | H) and p(1 | EC) = p(1 | C) which can be read directly from our table.


Problem 2:
----------

See vtag


Problem 3:
----------

See vtag


Problem 4:
----------

After applying the one-count smoothing, I got the following results:

Tagging accuracy (Viterbi decoding): 94.09% (known: 96.82% novel: 65.80%)
Perplexity per Viterbi-tagged test word: 864.905

This represents an improvement of 1.61 percentage points over the baseline in overall accuracy, 0.83 points in known word accuracy, and 9.73 points in novel word accuracy.

We also improved by 712.594 in perplexity, which is very substantial.


Problem 5:
----------

After adding posterior decoding, I got the following results for entest:

Tagging accuracy (Viterbi decoding): 94.09% (known: 96.81% novel: 65.84%)
Perplexity per Viterbi-tagged test word: 864.609
Tagging accuracy (posterior decoding): 94.16% (known: 96.84% novel: 66.41%

As we can see, the posterior decoder does just marginally better overall, with hardly any difference for known words, and about a 0.57 percentage point improvement for novel words. The program as a whole runs a bit slower but not by enough to suggest that the posterior decoding is taking significantly longer than the Viterbi tagging.

On the ice-cream set, I noticed that my implementation gives exactly the same output as the Excel sheet on the first iteration. However, this output is actually not as good as the Viterbi tagger, oddly enough. It seems that this is an outlier possibly caused by a small data set, though, as forwards-backwards did outperform Viterbi on the entest data.


Problem 6:
----------


